{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef9ae35-d5f7-4b02-8a32-7fb05150424d",
   "metadata": {},
   "source": [
    "# **traumaScanner**\n",
    "\n",
    "**Date:** 09-13-2023\n",
    "\n",
    "**Author**: Meghan Hutch\n",
    "\n",
    "**Objective**: Identify patients with post-traumatic brain hemorrhage via the use of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98b7da-ff0c-4cf8-97d0-2d27fa6615c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56469a-bf95-47e4-98f7-c7335c6ed052",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00655fff-906a-4ffd-8a07-0298711f7a03",
   "metadata": {},
   "source": [
    "### Load tbiExtractor_suid dataset\n",
    "\n",
    "We will load in the dataset that has been prepared in the `notebooks/text-mining/merge_tbiExtractor_suid.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d043c68-8c4f-478c-9111-a775667e3704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataframe with notes to evaluate\n",
    "#tbiExtractor_results_all = pd.read_csv('/share/nubar/Neurotrauma/hematoma_expansion/data/processed_data/tbiExtractor_suid.csv')\n",
    "\n",
    "radiology_reports_df = pd.read_csv('/share/nubar/Neurotrauma/hematoma_expansion/tbi_cohort/data/processed/suid_rad_reports.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6477bb6-9b74-47e5-b129-cdffdd6254e7",
   "metadata": {},
   "source": [
    "## Data pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d04a45-9ab4-437c-aba5-ab688d2eb099",
   "metadata": {},
   "source": [
    "**Create a new cleaned up dataframe with only initial analysis variables of interest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535fa44-8e58-43d6-819c-2edefe04a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiology_reports_df_analyze = radiology_reports_df[['order_reason', 'report_num_temp', 'unique_study_id', 'report']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17e67f-95e0-4ce1-9959-6c79b1541d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review number of unique reports and study ids\n",
    "print(len(radiology_reports_df_analyze[['report_num_temp', 'unique_study_id']].drop_duplicates()))\n",
    "print(len(radiology_reports_df_analyze[['unique_study_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54142173-ace0-4346-aa20-944fe29d104b",
   "metadata": {},
   "source": [
    "**Append `order_reason` text to `report`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274e8bd-5da6-4e47-b29d-5cfcbd1482dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(radiology_reports_df_analyze[radiology_reports_df_analyze['order_reason'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fae849e-744d-4021-b772-ce3a3ac757f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add order_reason to the report; we use fillna() to handle the handful of missing order_reasons (n = 4)\n",
    "#radiology_reports_df_analyze['report'] = 'Order reason: ' + radiology_reports_df_analyze['order_reason'].fillna('') + radiology_reports_df_analyze['report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b60cd-8af2-42b5-b52f-cb56d078166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(radiology_reports_df_analyze[radiology_reports_df_analyze['report'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05673b23-9ba7-4bde-907a-19afb543c247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "radiology_reports_df_analyze[['order_reason', 'report_num_temp', 'unique_study_id', 'report']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34b59f-98cd-4c8f-8285-b7a530c33335",
   "metadata": {},
   "source": [
    "## Find post-traumatic hemorrhage reports\n",
    "\n",
    "This next set of code is devised from the iterative testing performed in `notebooks/text-mining/findTBI_reports.ipynb`. Here, we will employ a series of regular expressions (regex) to identify radiology reports with post-traumatic hemorrhage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c223e2-f552-4904-93f2-c21921b6b269",
   "metadata": {},
   "source": [
    "### Identify potential trauma patients\n",
    "\n",
    "First, we will identify patients who are being evaluated after a traumatic injury. This list was originally composed with the keywords specified in `data/trauma/search_criteria.txt`. I added a few other words that appeared during my testing. I also have implemented a for-loop to help identify words that need to be exactly matched in order to avoid false positive matches (e.g. 'stab' returning reports with the word 'stable'; 'hit' returning reports with the word 'white'). I also removed words like 'injury' which was retrieving reports which were false positives for traumatic brain injury, and instead were matching on phrases such as 'ischemic injury' or 'post-operative injury'. \n",
    "\n",
    "*Note: Some of the words in the partial matching `trauma_targets` list could be moved to `exact_match_words` list, but my original method attempted to only perform partial matching which led to the identification of some false positive reports. Thus, any of the words that absolutely needed to be exact matches were listed in the `exact_match_words` list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f52b3-c514-4c77-b4da-18c3005e17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## partial text-matching\n",
    "# list of trauma related key-words for partial-text matching\n",
    "trauma_targets = ['trauma', 'fall', 'lacera', 'assault', 'gun', 'kick', 'gsw', 'punch',\n",
    "                  'shot', 'mva', 'mvc', 'bike', 'vehicle', 'vehicular', 'collision', 'automobile']\n",
    "\n",
    "# identify reports that have a partial-text match to at least one of the trauma_targets\n",
    "potential_tbi_trauma = radiology_reports_df_analyze[radiology_reports_df_analyze['report'].str.contains('|'.join(trauma_targets), case = False, regex = True)]\n",
    "print(potential_tbi_trauma[['unique_study_id']].nunique())\n",
    "\n",
    "## exact-text matching\n",
    "# initialize an empty DataFrame to store the results\n",
    "exact_results_df = pd.DataFrame()\n",
    "\n",
    "# Perform exact matches for words in exact_match_words list\n",
    "# note: if we want exact word match - \\b \\b adds boundaries. E.g. \\bstab\\b will return 'stab' and not 'stable'\n",
    "# added additional words 'brain injury', 'head injury', 'fell', 'stabbing' based on prior sensitivity analyses\n",
    "exact_match_words = ['auto', 'accidents', 'accident', 'hit', 'stab', 'stabbed' 'stabbing', 'slip', 'slipped', 'struck', 'car', \n",
    "                     'brain injury', 'head injury', 'fell', 'contrecoup', 'contracoup', 'coup', 'tSAH']\n",
    "for word in exact_match_words:\n",
    "    exact_match_result = radiology_reports_df_analyze[radiology_reports_df_analyze['report'].str.contains(fr'\\b{word}\\b', case=False, regex=True)]\n",
    "    exact_results_df = pd.concat([exact_results_df, exact_match_result])\n",
    "\n",
    "# list the dataframe of reports that matched our exact or partial-text matching criteria\n",
    "report_matches = [potential_tbi_trauma, exact_results_df]\n",
    "\n",
    "# combine dataframes and drop any duplicated reports which may appear in both\n",
    "potential_tbi_trauma_reports = pd.concat(report_matches).drop_duplicates()\n",
    "\n",
    "# print out sample size of patients/reports that were matched\n",
    "print(len(potential_tbi_trauma_reports))\n",
    "print(potential_tbi_trauma_reports['unique_study_id'].nunique())\n",
    "print(len(potential_tbi_trauma_reports[['unique_study_id', 'report_num_temp']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f165920-0670-47e5-a1f7-2a83962c199e",
   "metadata": {},
   "source": [
    "**Create functions for text matching**\n",
    "\n",
    "The following functions will be applied to our corpus of radiology reports. The function `report_has_required_words()` takes in the unstructured radiology text report. Next it splits up each sentence of the report and uses the `has_all_required_words()` function to iteratively evaluate whether the list of target words (`pattern_sets`) is found in the sentence. If a sentence with a target word is found, the entire report (or row of the dataframe) will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90139c6a-6bc4-4c5d-9beb-e9b1983ee6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to apply regex; created with help from chatgpt\n",
    "\n",
    "# Function to check if a sentence contains all required words from any pattern set\n",
    "def has_all_required_words(sentence, pattern_sets):\n",
    "    return any(all(re.search(pattern, sentence) for pattern in pattern_set) for pattern_set in pattern_sets)\n",
    "\n",
    "# Function to check if a report contains at least one sentence with all required words from any pattern set\n",
    "def report_has_required_words(report, pattern_sets):\n",
    "    sentences = report.split('.')\n",
    "    return any(has_all_required_words(sentence, pattern_sets) for sentence in sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f585358-7383-4860-92e2-b1311005bae9",
   "metadata": {},
   "source": [
    "### Remove likely/uncertain false positives for trauma\n",
    "\n",
    "When devising this method, we noticed that some reports were returned if there was a phrase such as 'No history of trauma'. Previously, I thought the downstream regex for stratifying patients with and without hemorrhage would ensure these patients were not included in our post-traumatic hemorrhage cohort, but sometimes, these patients have what appears to be spontaneous / post-operative hemorrhage from non-trauma sources. \n",
    "\n",
    "Thus, this step attempts to ensure that we only maintain patients who we are highly confident are being evaluated for traumatic brain injury.\n",
    "\n",
    "***Note***: \n",
    "\n",
    "This method will remove some patients who did have a trauma type injury as mentioned. For example, a patient may have a MVC, however, the radiologist might write: \"no evidence of recent traumatic injury\". This is okay, because the overall objective is to identify patients with post-traumatic hemorrhage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108a85a-84f0-4e03-9520-1485ed68238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_sets = [\n",
    "    # the following regex employs a positive lookahead assertion; \n",
    "    # it makes sure to remove sentences where No comes before the words (recent|known|history|...), which also come before (trauma|traumatic)\n",
    "    # see additional regex notes towards bottom of notebook\n",
    "    [r'(?i)\\b(no|negative)\\b(?=.*(\\b(recent|known|obvious|history|reported|definite)\\b.*\\b(trauma|traumatic)\\b))'],\n",
    "    [r'(?i)\\b(vascular accident)\\b'],\n",
    "    [r'(?i)\\b(anoxic brain injury)\\b'],\n",
    "]\n",
    "\n",
    "potential_trauma_fp = potential_tbi_trauma_reports[potential_tbi_trauma_reports['report'].apply(report_has_required_words, pattern_sets=pattern_sets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c6c6d-c583-46a6-ad6a-6b941859561d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(potential_trauma_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533f902-5be8-498f-bf6b-92ffd7b7a775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# can review a random sample of the reports we will exclude \n",
    "#potential_trauma_fp.sample(n = 5, random_state=1308).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138a586-96ff-49d5-9888-88db8feabc54",
   "metadata": {},
   "source": [
    "**Remove the likely FP reports from the `potential_tbi_trauma_reports`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996de0f9-1877-42ed-8a3d-f228bb0cf1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the reports that match the regex pattern above\n",
    "potential_tbi_trauma_reports = potential_tbi_trauma_reports[~(potential_tbi_trauma_reports['report'].apply(report_has_required_words, pattern_sets=pattern_sets))]\n",
    "\n",
    "# print out sample size of patients/reports that were matched\n",
    "print(len(potential_tbi_trauma_reports))\n",
    "print(potential_tbi_trauma_reports['unique_study_id'].nunique())\n",
    "print(len(potential_tbi_trauma_reports[['unique_study_id', 'report_num_temp']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d5716-1c1e-4928-97dd-2082dd8d8320",
   "metadata": {},
   "source": [
    "### Identify post-traumatic hemorrhage\n",
    "\n",
    "The set of regular expressions curated below aims to leverage common phrases and templated language that radiologists use to describe the ***absence*** of hemorrhage or intrancranial abnormalities. This list was devised during my chart reviews and sensitivity analyses as I reviewed reports and saw the type of language commonly repeated to describe the absence of any hemorrhage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4e2c4-e630-4ca5-8925-b6625728e4af",
   "metadata": {},
   "source": [
    "**Regex notes:**\n",
    "\n",
    "*copied from chatgpt, but evaluated their validity with other online resources to guide my understanding*\n",
    "\n",
    "Let's take the following regex as an example:\n",
    "\n",
    "`r'(?i)\\b(no|negative)\\b(?!.*\\b(new|additional)\\b).*?\\b(evidence|acute|intracranial)\\b.*?\\b(trauma|traumatic|hemorrhage|hematoma)\\b'`\n",
    "\n",
    "* `r`: This is a Python string prefix that indicates that the following string should be treated as a raw string, which is often used with regular expressions.\n",
    "\n",
    "* `(?i)`: This is a regex flag that makes the pattern case-insensitive. It allows the regex to match both uppercase and lowercase letters. So, \"No,\" \"no,\" \"NO,\" \"Negative,\" \"negative,\" etc., will all be matched.\n",
    "\n",
    "*Note: This looks to affect all following words; not just the capture group (no | negative) right after it*\n",
    "\n",
    "* `\\b`: This is a word boundary anchor. It matches the position where a word starts or ends. It ensures that the words \"no\" or \"negative\" are matched as whole words, not as part of another word. \n",
    "\n",
    "* `\\b(no|negative)\\b`: This part is a word boundary (`\\b`) followed by a non-capturing group `(?:...)` that matches either \"no\" or \"negative.\" The word boundary ensures that it matches these words as whole words. The `|` symbol acts as an OR operator, allowing either word to match.\n",
    "\n",
    "* `(?!.*\\b(new|additional)\\b)`: This is a negative lookahead assertion `(?!...).` It checks for the absence of the following condition:\n",
    "\n",
    "   *  `.*\\b(new|additional)\\b`: It matches any characters `(.*)` that contain the whole words \"new\" or \"additional\" (with word boundaries). If this condition is met, the negative lookahead fails, meaning that sentences containing \"new\" or \"additional\" are excluded.\n",
    "   \n",
    "*  `.*?`: This part matches any characters (as few as possible) between the preceding and following parts.\n",
    "\n",
    "* `\\b(evidence|acute|intracranial)\\b`: This part matches \"evidence,\" \"acute,\" or \"intracranial\" as whole words. The `\\b` word boundaries ensure they are matched entirely.\n",
    "\n",
    "`.*?`: Again, this part matches any characters (as few as possible) before the next specified term.\n",
    "\n",
    "`\\b(trauma|traumatic|hemorrhage|hematoma)\\b`: This part matches one of the specified medical terms as whole words, including \"trauma,\" \"traumatic,\" \"hemorrhage,\" or \"hematoma.\" The `\\b` word boundaries ensure that these terms are matched entirely.\n",
    "\n",
    "In summary, this regular expression is designed to match sentences where:\n",
    "\n",
    "* \"no\" or \"negative\" appears as a whole word at the beginning.\n",
    "* \"new\" or \"additional\" is excluded from the entire sentence.\n",
    "* \"evidence,\" \"acute,\" or \"intracranial\" appears before \"trauma,\" \"traumatic,\" \"hemorrhage,\" or \"hematoma.\"\n",
    "* The words are matched as complete words due to the use of word boundaries `\\b`.\n",
    "* This regex is intended to identify specific patterns in text data, such as medical reports, where these conditions need to be met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8a688-a149-432a-9630-6c8229fe95e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pattern_sets = [\n",
    "    #[r'(?i)\\b(remote)\\b(?=.*(\\b(trauma)\\b))'],\n",
    "    [r'(?i)\\b(unremarkable|negative)\\b(?=.*(\\b(exam|head\\sCT|CT|study)\\b))']\n",
    "]\n",
    "\n",
    "potential_trauma_fp = potential_tbi_trauma_reports[potential_tbi_trauma_reports['report'].apply(report_has_required_words, pattern_sets=test_pattern_sets)]\n",
    "print(len(potential_trauma_fp))\n",
    "potential_trauma_fp.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3c916-131f-4a4a-9891-aff495847520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regular expression pattern for each of the expressions\n",
    "# this set of patterns will be applied to each sentence of each report\n",
    "pattern_sets = [\n",
    "    # detects sentences that contains the word no or negative before at least one of the following words (evidence|acute|negative), which also occur before the words (trauma|traumatic|hemorrhage|hematoma)\n",
    "    # importantly, we ensure that if the word 'new' or 'additional' is present that we do not exclude it. Often these words are part of phrases that suggest there is hemorrhage\n",
    "    [r'(?i)\\b(no|negative)\\b(?!.*\\b(new|additional)\\b).*?\\b(evidence|acute|intracranial)\\b.*?\\b(trauma|traumatic|hemorrhage|hematoma)\\b'],\n",
    "    # detect matches without the previous qualifier (evidence|acute|intracranial) from above\n",
    "    [r'(?i)\\b(no|negative)\\b(?:(?!(\\bnew\\b|\\badditional\\b)).)*\\b(trauma|traumatic|hemorrhage|hematoma)\\b'],\n",
    "    # detects no acute findings\n",
    "    # note: adding 'intracranial' as a prefix before 'findings' leads to two false positives; thus will not combine the two subsequent regex\n",
    "    [r'(?i)\\b(no|negative)\\b(?=.*(\\b(acute)\\b.*\\b(findings)\\b))'],\n",
    "    # detects no CT abnormalities\n",
    "    [r'(?i)\\b(no|negative)\\b(?=.*(\\b(intracranial|acute)\\b.*\\b(abnormality|abnormalities)\\b))'],\n",
    "    # detects no abnormality\n",
    "    [r'(?i)\\b(no|negative)\\b(?=.*(\\b(abnormality|abnormalities)\\b))'],\n",
    "    # detects negative|unremarkable head CT / negative finding\n",
    "    [r'(?i)\\b(unremarkable|negative)\\b(?=.*(\\b(exam|head\\sCT|CT|study)\\b))'],\n",
    "    # detects normal exam\n",
    "    [r'(?i)\\b(normal study|normal CT|normal head CT|normal exam|normal head CT|normal noncontrast|normal plain)\\b'],\n",
    "    # detects no intracranial process - including acute might return us old scans that we may want to keep\n",
    "    [r'(?i)\\b(no|negative|without\\sevidence)\\b(?=.*(\\b(intracranial)\\b.*\\b(process|pathology)\\b))'],\n",
    "    # remove exact phrases of the following:\n",
    "    [r'(?i)\\b(without acute intracranial abnormality)\\b'],\n",
    "    [r'(?i)\\b(without evidence for acute abnormality)\\b'],\n",
    "    [r'(?i)\\b(without acute abnormality)\\b']\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9067e-ae47-4b92-be81-3b2bd54125b6",
   "metadata": {},
   "source": [
    "#### testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68f4dc-2dd8-437d-836d-2bb5c23b7fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### code provided with help of chatgpt\n",
    "### this will print out the \n",
    "import re\n",
    "\n",
    "# Function to check if a sentence contains all required words from any pattern set\n",
    "def has_all_required_words_debug(sentence, pattern_sets):\n",
    "    #print(sentence)\n",
    "    for pattern_set in pattern_sets:\n",
    "        for pattern in pattern_set:\n",
    "            if re.search(pattern, sentence):\n",
    "                return pattern  # Return the matching pattern\n",
    "    return None  # Return None if no pattern matched\n",
    "\n",
    "# Function to check if a report contains at least one sentence with all required words from any pattern set\n",
    "def report_has_required_words_debug(report, pattern_sets):\n",
    "    sentences = report.split('.')\n",
    "    for sentence in sentences:\n",
    "        pattern = has_all_required_words_debug(sentence, pattern_sets)\n",
    "        if pattern:\n",
    "            print(sentence)\n",
    "            return pattern  # Return the matching pattern\n",
    "    return None  # Return None if no pattern matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe64ba-b565-4c3c-86e8-f5c9c17d1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fake reports to text different regex\n",
    "d = {'report': ['there are no new acute of hemorrhage or new extra-axial fluid identified.']}\n",
    "text_ex = pd.DataFrame(data = d)\n",
    "#print(text_ex)\n",
    "\n",
    "text_ex['report'].apply(report_has_required_words_debug, pattern_sets=pattern_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6574ed1-55c7-49f7-b769-1c5bc658dffb",
   "metadata": {},
   "source": [
    "#### Apply regex statements to identify patients without post-traumatic hemorrhage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924fe483-f3a8-434a-8d86-4ec63886fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will identify all reports with no hemorrhage\n",
    "# for this process; we will create an abbreviated version of the larger dataset `potential_tbi_trauma_reports` and apply our `pattern_set`\n",
    "potential_tbi_no_hem = potential_tbi_trauma_reports[['unique_study_id', 'report_num_temp', 'report']].drop_duplicates()\n",
    "potential_tbi_no_hem = potential_tbi_trauma_reports[potential_tbi_no_hem['report'].apply(report_has_required_words, pattern_sets=pattern_sets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52436a6b-083e-40a6-a430-d19086145cbf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "potential_tbi_no_hem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d647b3e-35e0-4d37-8cff-3730d00d3de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(potential_tbi_no_hem[['unique_study_id']].drop_duplicates()))\n",
    "print(len(potential_tbi_no_hem[['unique_study_id', 'report_num_temp']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87351989-73fd-42a0-8c5f-3adf02f94c32",
   "metadata": {},
   "source": [
    "#### Return patients with likely hemorrhage\n",
    "\n",
    "Next, we will merge the patients with potentially no hemorrhage, with the original `potential_tbi_trauma_reports` dataset, in order to return the patients with likely post-traumatic hemorrhage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e07f4-0534-4928-b348-16fb9ea98ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "potential_tbi_hem = pd.merge(potential_tbi_trauma_reports[['unique_study_id', 'report_num_temp', 'report']].drop_duplicates(), \n",
    "                             potential_tbi_no_hem[['unique_study_id', 'report_num_temp', 'report']].drop_duplicates(), \n",
    "                             indicator = True, how = 'left').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "\n",
    "\n",
    "# before dropping any rows\n",
    "# note: adding up patient ID of the potential_tbi_hem and the potential_tbi_no_hem is not a good check that our merging worked because patients could end up in both groups depending on how/when there reports were taken\n",
    "# note: however, the number of reports from both new dataframes should match original potential_tbi_trauma_reports\n",
    "print(len(potential_tbi_hem[['unique_study_id']].drop_duplicates()))\n",
    "print(len(potential_tbi_hem[['unique_study_id', 'report_num_temp']].drop_duplicates()))\n",
    "\n",
    "print(len(potential_tbi_no_hem[['unique_study_id']].drop_duplicates()))\n",
    "print(len(potential_tbi_no_hem[['unique_study_id', 'report_num_temp']].drop_duplicates()))\n",
    "\n",
    "print(len(potential_tbi_trauma_reports[['unique_study_id']].drop_duplicates()))\n",
    "print(len(potential_tbi_trauma_reports[['unique_study_id', 'report_num_temp']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273c33f-31a0-4d84-a50c-5d58512b31c3",
   "metadata": {},
   "source": [
    "**Rescue reports**\n",
    "\n",
    "Next, we will rescue reports that may have been excluded do the regex rules that excluded reports containing sentences such as:\n",
    "\n",
    "- \"No evidence of additional hemorrhage\"\n",
    "- \"No interval change in the amount of hemorrhage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917f787-86a2-4b53-b032-01d65a25f438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate whether we are excluding patinets with \"no additional hemorrohage\"\n",
    "# try ensuring \"No\" prior to (additional|change|decrease) in (hematoma|hemorrhage|contusion)\n",
    "# our regex also enables capturing `change, changed, changing (and similar matching with increase and decrease)\n",
    "# we will also include the word hemorrhage; in this instance, we want to try and identify any reports that may have been false negative for post-traumatic hemorrhage\n",
    "# contusion seems pretty sensitive for brain trauma/potential hemorrhage \n",
    "pattern_sets = [\n",
    "    [r'(?i)\\b(no|negative)\\b(?=.*(\\b(additional|(chang|increas|decreas)(e|ed|ing))\\b.*\\b(hematoma|hemorrhage|hemorrhagic|contusion)\\b))']\n",
    "]\n",
    "\n",
    "rescue_additional_reports = potential_tbi_no_hem[potential_tbi_no_hem['report'].apply(report_has_required_words, pattern_sets=pattern_sets)]\n",
    "print(len(rescue_additional_reports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea51a4-e041-4c50-b693-7b9b24442ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rescue_additional_reports.sample(n = 5, random_state=1308).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27a871-cc97-4b22-b685-1746eb1fbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resave the potential_tbi_no_hem to remove the rescued reports\n",
    "potential_tbi_no_hem_v2 = potential_tbi_no_hem[~potential_tbi_no_hem['report'].apply(report_has_required_words, pattern_sets=pattern_sets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe04650-811b-4a8f-8060-4f92e86b23a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing whether secondary hemorrhage can be a key phrase to identify post-traumatic hemorrhage reports\n",
    "test_pattern_sets = [\n",
    "   # [r'(?i)\\b(secondary)\\b(?=.*(\\b(hematoma|hemorrhage|contusion)\\b))'],\n",
    "    #[r'(?i)\\b(subarachnoid|subdural|intraparenchymal|intraventricular)\\b(?=.*(\\b(hematoma|hemorrhage|contusion)\\b))'],\n",
    "    [r'(?i)\\b(contracoup)\\b'] \n",
    "]\n",
    "\n",
    "potential_trauma_fn = potential_tbi_no_hem_v2[potential_tbi_no_hem_v2['report'].apply(report_has_required_words, pattern_sets=test_pattern_sets)]\n",
    "print(len(potential_trauma_fn))\n",
    "potential_trauma_fn.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6770f-06cd-453c-a645-bd0163238808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add reports to the `potential_tbi_hem` dataset\n",
    "tbi_reports_list = [potential_tbi_hem, rescue_additional_reports]\n",
    "\n",
    "post_traumatic_hem_reports = pd.concat(tbi_reports_list)\n",
    "len(post_traumatic_hem_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76d9dc-7718-48fe-9f13-8566dad29347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate whether we've correctly concatenated the extra reports\n",
    "len(potential_tbi_hem) + len(rescue_additional_reports) == len(post_traumatic_hem_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a2ea2-6bcb-4f90-9602-369ffa079ca6",
   "metadata": {},
   "source": [
    "#### Tally total number of reports and patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8c975-6280-417c-9499-2c10a203e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(post_traumatic_hem_reports[['unique_study_id']].drop_duplicates()))\n",
    "print(len(post_traumatic_hem_reports[['unique_study_id', 'report_num_temp']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454832c-5944-4407-a6fc-fbd8aaa546a8",
   "metadata": {},
   "source": [
    "## Chart Review Validation\n",
    "\n",
    "Next, we will randomly sample reports in order to perform a manual chart review and validation of this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77137986-085b-42cf-9073-00d8e7f08b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to stratify reports by post_traumatic_hemorrhage, potential_tbi_no_hem\n",
    "post_traumatic_hem_reports_to_validate = post_traumatic_hem_reports.sample(n = 50, random_state=1308).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525093b2-2ab8-4a4e-abea-8209b4ea6041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#post_traumatic_hem_reports_to_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a643f-8097-421c-b676-57e7e6fcfd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# automatically save a file with the current date and time\n",
    "#date_to_save = f'/share/nubar/Neurotrauma/hematoma_expansion/data/chart_review/{datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")}_potential_tbi_to_review.csv'\n",
    "#post_traumatic_hem_reports_to_validate.to_csv(date_to_save, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d6295-ce1b-401b-8879-4dfd43e92b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_post_traumatic_hem_reports_to_validate = potential_tbi_no_hem_v2.sample(n = 50, random_state=1308).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2343f2-7ad4-4603-b3a3-8074c66861b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#non_post_traumatic_hem_reports_to_validate.to_csv('/share/nubar/Neurotrauma/hematoma_expansion/data/chart_review/2023.10.10_potential_tbi_non_hem_chart_review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c286841-0c6f-4750-82cf-63295839d9a5",
   "metadata": {},
   "source": [
    "#### Save list of the identified patients and reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ab365-63a3-485f-b181-c10671a1fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_traumatic_hem_reports_unique = post_traumatic_hem_reports[['unique_study_id', 'report_num_temp']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c34bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_traumatic_hem_reports.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce4132-8ea1-4dbc-8c41-22249060fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically save a file with the current date and time\n",
    "#date_to_save = f'/share/nubar/Neurotrauma/hematoma_expansion/data/processed_data/{datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")}_potential_trauma_patient_reports.csv'\n",
    "\n",
    "date_to_save = f'/share/nubar/Neurotrauma/hematoma_expansion/tbi_cohort/data/processed/{datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")}_potential_trauma_patient_reports.csv'\n",
    "post_traumatic_hem_reports_unique.to_csv(date_to_save, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42940d3f-2697-47f8-a951-4c8dc83548ef",
   "metadata": {},
   "source": [
    "**Evaluate differences from previous iteration**\n",
    "\n",
    "The below code facilitates checking the addition or subtraction of reports based on different changes to regex / filtering rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547f7de-2d4c-48a4-9e4e-c12fb86bc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbiExtractor_results_all = pd.read_csv('/share/nubar/Neurotrauma/hematoma_expansion/data/processed_data/tbiExtractor_suid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e2374-e50c-4b10-8ebe-ec05148f3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a previous list of reports \n",
    "#post_traumatic_hem_reports_unique_orig = pd.read_csv('/share/nubar/Neurotrauma/hematoma_expansion/data/processed_data/20231017_1635_potential_trauma_patient_reports.csv')\n",
    "post_traumatic_hem_reports_unique_orig = pd.read_csv('/share/nubar/Neurotrauma/hematoma_expansion/data/processed_data/20231024_1615_potential_trauma_patient_reports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8bb55-5fc6-4767-a34c-cd5afabccc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check differences between the current and past iteration\n",
    "diff_reports = pd.merge(post_traumatic_hem_reports_unique_orig, \n",
    "                        post_traumatic_hem_reports_unique,\n",
    "                        how = 'outer',\n",
    "                        indicator = True)\n",
    "\n",
    "# right_only lists the reports that were most recently identified in the current, but not the previous iteration\n",
    "# left_only lists the reports that are no longer included in the current iteration\n",
    "print(len(diff_reports[diff_reports['_merge'] == 'left_only']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31dc67-1c88-4108-888b-6611022d1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select either `left_only` or `right_only` to review\n",
    "reports_removed = diff_reports[diff_reports['_merge'] == 'left_only']\n",
    "\n",
    "# merge discordant reports with the tbiExtractor results in order to review the radiology report\n",
    "diff_reports_to_eval = pd.merge(radiology_reports_df_analyze[['unique_study_id', 'order_reason', 'report_num_temp', 'report']].drop_duplicates(),\n",
    "         reports_removed,\n",
    "         how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435079bc-be03-43f2-9f4b-a5be5b574ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_reports_to_eval.iloc[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e809bc-4f30-4cc6-8a5a-c3d5d549f895",
   "metadata": {},
   "source": [
    "**Which patients were newly identified as trauma patients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded17f77-edba-492f-9b77-0c4279d810c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check differences between the current and past iteration\n",
    "diff_id_reports = pd.merge(post_traumatic_hem_reports_unique_orig[['unique_study_id', 'report_num_temp']].drop_duplicates(), \n",
    "                        post_traumatic_hem_reports_unique[['unique_study_id', 'report_num_temp']].drop_duplicates(),\n",
    "                        how = 'outer',\n",
    "                        indicator = True)\n",
    "\n",
    "# right_only lists the reports that were most recently identified in the current, but not the previous iteration\n",
    "# left_only lists the reports that are no longer included in the current iteration\n",
    "print(len(diff_id_reports[diff_id_reports['_merge'] == 'right_only']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59062f60-33e2-4859-851a-482436ef04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select either `left_only` or `right_only` to review\n",
    "reports_id_removed = diff_id_reports[diff_id_reports['_merge'] == 'right_only']\n",
    "\n",
    "# merge discordant reports with the tbiExtractor results in order to review the radiology report\n",
    "diff_id_reports_to_eval = pd.merge(radiology_reports_df_analyze[['unique_study_id', 'order_reason', 'report_num_temp', 'report']].drop_duplicates(),\n",
    "         reports_id_removed,\n",
    "         how = 'inner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbi-env",
   "language": "python",
   "name": "tbi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
